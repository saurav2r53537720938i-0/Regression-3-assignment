{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3572e4-a35a-44f0-97a8-b00d2581b051",
   "metadata": {},
   "source": [
    "Least squares regression and ordinary least squares(OLS) regression are often used interchangeably but there a subtle diffrence between them:\n",
    "1. Ordinary Least Square:In ols regression the goal is to minimize the sum of the squared diffrence between the predicted value obtained from the regression equation.\n",
    "2. Least Squares Regression:Least square regression is a broader term that encompasses various method for fitting a  regression line to data by minimizing the sum of the squared diffrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118f92a-bdd6-4645-a771-3b0d6ede493a",
   "metadata": {},
   "source": [
    "Ridge regression a regularization technique used in linear regression is based of the following assumptions:\n",
    "1. Linearity\n",
    "2. No multicollinearity\n",
    "3. Independent errors\n",
    "4. Normality of errors\n",
    "5. Homoscedasicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b918683-8c1f-41a2-b19c-2c270fc63334",
   "metadata": {},
   "source": [
    "Selecting the value of the tuning paramter lambda in ridge regression often involves techniques like cross-validation or grid search cross validation involves splitting the dataset into multiple subsets and evaluating its performance to choose the optimal lambda value that minimize predictions error.Grid search involves trying diffrent \n",
    "lambda values and evaluating the model's performance on a validation set to determine the best lambda value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b6f93-bed9-4abb-9ee4-a2ec0b68e265",
   "metadata": {},
   "source": [
    "Yes,ridge regression can be used for feature slection to some extent.Ridge regression penalizes the cofficent of the feature,pushing them towards zero but not exactly zero which allows it to shrink cofficent rather than setting \n",
    "them to zero entirely like in lasso regression can however the magnitiude of the cofficent in ridge regression can still provide information about the important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c40c4-1120-44a4-aba8-34088c84e95f",
   "metadata": {},
   "source": [
    "The Ridge Regression model tends to perform better than ordinary least squares regression when multicollinerity is prsent because it mitigate the issue by adding a penality term to the regression cofficent this penalty terms helps to stablize the cofficent reducing their senstivity to multicollinearity and yielding more reliable estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee720-e31a-4d6f-899c-9e1601dc3355",
   "metadata": {},
   "source": [
    "Yes,Ridge Regression can handle both categorical and continous independent variable typically need to be transformed into dummy variable be feature also as series of binary variable to be reprsented as a series of binary  variable in the ridge regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af5b2f-3f99-46d0-a419-4d0a670f6f6c",
   "metadata": {},
   "source": [
    "Interpreting Ridge Regression involves understanding how the cofficent are affected by penalty term introduce to combat multicollinearity:\n",
    "1. Magnitiude of cofficents\n",
    "2. Shrinkage\n",
    "3. Variable importance\n",
    "4. Stablity\n",
    "5. Model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0f23c-6086-4e10-baf8-1a6e965532d9",
   "metadata": {},
   "source": [
    "yes, Ridge Regression can be useed for time series data anylsis.however it impo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
